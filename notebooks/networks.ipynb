{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "tf_env_gpu",
      "language": "python",
      "name": "tf_env_gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Copy of Copy of documents_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:50:47.759753Z",
          "start_time": "2020-03-05T14:50:45.744863Z"
        },
        "id": "IO3xMos3JjSt"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import statistics\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Conv1D, Dropout, Embedding, BatchNormalization, Activation\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMc6GXcI7ZmI",
        "outputId": "68e6f5de-796a-4cf3-83e0-9fff39d9dcab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pupMiV7xK7Xp"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Conv1D, Dropout, Embedding\r\n",
        "from tensorflow.keras.preprocessing import sequence\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "import nltk\r\n",
        "import tensorflow_hub as hub\r\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji7H_igvmKUz"
      },
      "source": [
        "# from gensim.parsing import preprocessing\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKXUOYR_pkDU"
      },
      "source": [
        "\n",
        "\n",
        "# nltk.download('punkt')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj2Rp_phJjSz"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9HbU9ZUJjS4"
      },
      "source": [
        "#PATH TO DATA FISIER test_all.csv pe care l-am incercat pe teams\r\n",
        "\r\n",
        "path_to_file = '/content/drive/MyDrive/dataset_SRI/test_all.csv'\r\n",
        "\r\n",
        "df = pd.read_csv(path_to_file)\r\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e1sh7m61_-A"
      },
      "source": [
        "df_hyper_false = df[df.hyperpartisan == False]\r\n",
        "df_hyper_true = df[df.hyperpartisan == True]\r\n",
        "\r\n",
        "# text_values_false = df_hyper_false.body.values\r\n",
        "# text_values_true = df_hyper_true.body.values\r\n",
        "tragets_false = df_hyper_false.hyperpartisan.values\r\n",
        "tragets_true = df_hyper_true.hyperpartisan.values"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18227BEIuacg"
      },
      "source": [
        "dataset_size_div_by_2 = 25_000\r\n",
        "\r\n",
        "# text_values_false = text_values_false[:dataset_size_div_by_2]\r\n",
        "tragets_false = tragets_false[:dataset_size_div_by_2]\r\n",
        "#############################\r\n",
        "# text_values_true = text_values_true[:dataset_size_div_by_2]\r\n",
        "tragets_true = tragets_true[:dataset_size_div_by_2]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# text = np.concatenate((text_values_false, text_values_true), axis=0)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "labels = np.concatenate((tragets_false, tragets_true), axis=0)\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJaM1g7z3_iw"
      },
      "source": [
        "del df\r\n",
        "del df_hyper_false\r\n",
        "del df_hyper_true"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "e21D8jtOJjS7"
      },
      "source": [
        "# from nltk.tokenize import sent_tokenize\n",
        "# from nltk.tokenize import word_tokenize\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOrFMtpLJjS_"
      },
      "source": [
        "# sent_text = []\r\n",
        "# new_labels = []\r\n",
        "# for idx, txt in enumerate(text):\r\n",
        "#   if type(txt) == str:\r\n",
        "#     my_txt = txt.replace('\\n', ' ').lower()\r\n",
        "#     # my_txt = preprocessing.remove_stopwords(my_txt)\r\n",
        "#     # my_txt = preprocessing.strip_tags(preprocessing.strip_numeric(my_txt))\r\n",
        "#     sents = sent_tokenize(txt)\r\n",
        "#     word_level_sents = []\r\n",
        "#     for sent in sents:\r\n",
        "#       word_level_sents.append(word_tokenize(sent))\r\n",
        "#     sent_text.append(word_level_sents)\r\n",
        "#     new_labels.append(labels[idx])\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0JeDkw8f_gJ"
      },
      "source": [
        "# size_sent = []\r\n",
        "# size_word = []\r\n",
        "# for txt in sent_text:\r\n",
        "#   size_sent.append(len(txt))\r\n",
        "#   for val in txt:\r\n",
        "#     size_word.append(len(val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7ou8e5L7hNw"
      },
      "source": [
        "# nb_of_tokens = 60\r\n",
        "# nb_of_sents = 30\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMzI5yiAg0Qf"
      },
      "source": [
        "# np.mean(size_sent), np.mean(size_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8uT6A7uuskz"
      },
      "source": [
        "# !pip install transformers\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mCUmLQEu6F-"
      },
      "source": [
        "# ####get new sentences :D:D:D:D:D\r\n",
        "# !pip install transformers\r\n",
        "# from transformers import BertTokenizer, TFBertModel\r\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n",
        "# model = TFBertModel.from_pretrained(\"bert-base-uncased\")\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xaDISaL4p93"
      },
      "source": [
        "# def create_bert_ds(sent_text):\r\n",
        "#   bert_encodings = []\r\n",
        "#   for idx2, a_news in enumerate(sent_text):\r\n",
        "#     if idx2 % 100 == 0:\r\n",
        "#       print(idx2)\r\n",
        "#       with open('/content/drive/MyDrive/dataset_SRI/embeddings', 'wb') as filehandle:\r\n",
        "#         pickle.dump(bert_encodings, filehandle)\r\n",
        "#     snt_good = []\r\n",
        "#     for idx, snt in enumerate(a_news):\r\n",
        "#       if idx == nb_of_sents:\r\n",
        "#         break\r\n",
        "#       snt_good.append(' '.join(snt))\r\n",
        "#     while len(snt_good) != nb_of_sents:\r\n",
        "#       snt_good.append('')\r\n",
        "#     assert(len(snt_good) == nb_of_sents)\r\n",
        "#     encoded_inputs = tokenizer(snt_good,  padding=True, truncation=True, return_tensors=\"tf\", max_length=60)\r\n",
        "#     output = model(encoded_inputs)\r\n",
        "#     bert_encodings.append(output[0])\r\n",
        "#   return bert_encodings\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMJOR5ZF516P"
      },
      "source": [
        "# encodings = create_bert_ds(sent_text)\r\n",
        "# #TO DO HERE TO SAVE FILE TO PATH\r\n",
        "# with open('/content/drive/MyDrive/dataset_SRI/embeddings', 'wb') as filehandle:\r\n",
        "#     # store the data as binary data stream\r\n",
        "#     pickle.dump(encodings, filehandle)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4f31n-AOYmQ"
      },
      "source": [
        "##################################\r\n",
        "#STOP RUN ALL HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQcYJs30WLtP"
      },
      "source": [
        "# from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# oov_token = '<UNK>'\r\n",
        "# pad_type = 'post'\r\n",
        "# trunc_type = 'post'\r\n",
        "# num_words = 35_000\r\n",
        "# tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\r\n",
        "# to_pass_token = []\r\n",
        "# for idx, doc in enumerate(sent_text):\r\n",
        "#   for sent in doc:\r\n",
        "#     to_pass_token.append(' '.join(sent))\r\n",
        "\r\n",
        "# tokenizer.fit_on_texts(to_pass_token)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIsl0uKlWdv6"
      },
      "source": [
        "# transf_seq = []\r\n",
        "# max_seq_lenght = 30\r\n",
        "# max_sent_lenght = 20\r\n",
        "# for idx, doc in enumerate(sent_text):\r\n",
        "#   transf_seq.append([])\r\n",
        "#   for idx, sent in enumerate(doc):\r\n",
        "#     if idx == max_sent_lenght:\r\n",
        "#       break\r\n",
        "#     train_sequences = tokenizer.texts_to_sequences([' '.join(sent)])\r\n",
        "#     train_padded = pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=max_seq_lenght)\r\n",
        "#     transf_seq[-1].append(train_padded[0])\r\n",
        "#   while len(transf_seq[-1]) != max_sent_lenght:\r\n",
        "#     train_padded = pad_sequences([[],[]], padding=pad_type, truncating=trunc_type, maxlen=max_seq_lenght)\r\n",
        "#     transf_seq[-1].append(train_padded[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OpmwCuPh0zX"
      },
      "source": [
        "# vocab_size = len(tokenizer.word_index) + 1\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiV0UP9Fq3JF",
        "outputId": "252dcf9a-8a40-402c-8409-550ca69ba663"
      },
      "source": [
        "%%time\r\n",
        "import os\r\n",
        "path_to_embeddings = '/content/drive/MyDrive/folder_me/embeddings/'\r\n",
        "\r\n",
        "def get_value(x):\r\n",
        "  return int(x[:-4].split('_')[-1])\r\n",
        "\r\n",
        "embedded_text = []\r\n",
        "all_files = os.listdir(path_to_embeddings)\r\n",
        "all_files = sorted(all_files, key = get_value)\r\n",
        "for file in all_files:\r\n",
        "  with open(path_to_embeddings + file, 'rb') as f:\r\n",
        "    current_embeds = pickle.load(f)\r\n",
        "    embedded_text.extend(current_embeds)\r\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4.81 s, sys: 6.34 s, total: 11.2 s\n",
            "Wall time: 3min 46s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVWSKxQ2q29z",
        "outputId": "5cbdc5e4-3c93-4e10-f556-e310c7bd9206"
      },
      "source": [
        "\r\n",
        "labels = labels[:49900]\r\n",
        "print(len(embedded_text), len(labels))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49900 49900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYzmbLRquVh5"
      },
      "source": [
        "from tensorflow.keras import backend as K\r\n",
        "nb_of_sents = 30\r\n",
        "def getModel(LSTM_CELL_SIZE):\r\n",
        "    in_id = tf.keras.layers.Input((nb_of_sents, 768), name=\"input_shape\")\r\n",
        "    \r\n",
        "    conv1 = Conv1D(filters=64,  kernel_size=2, padding='same')(in_id)    \r\n",
        "    batch_norm1  = BatchNormalization()(conv1)\r\n",
        "    actv1 = Activation('relu')(batch_norm1)\r\n",
        "    max_pool1 = tf.keras.layers.GlobalMaxPooling1D()(actv1)\r\n",
        "    \r\n",
        "    conv2 = Conv1D(filters=64,  kernel_size=3, padding='same')(in_id)    \r\n",
        "    batch_norm2  = BatchNormalization()(conv2)\r\n",
        "    actv2 = Activation('relu')(batch_norm2)\r\n",
        "    max_pool2 = tf.keras.layers.GlobalMaxPooling1D()(actv2)\r\n",
        "\r\n",
        "\r\n",
        "    conv3 = Conv1D(filters=64,  kernel_size=4, padding='same')(in_id)    \r\n",
        "    batch_norm3  = BatchNormalization()(conv3)\r\n",
        "    actv3 = Activation('relu')(batch_norm3)\r\n",
        "    max_pool3 = tf.keras.layers.GlobalMaxPooling1D()(actv3)\r\n",
        "\r\n",
        "\r\n",
        "    conv4 = Conv1D(filters=64,  kernel_size=5, padding='same')(in_id)    \r\n",
        "    batch_norm4  = BatchNormalization()(conv4)\r\n",
        "    actv4 = Activation('relu')(batch_norm4)\r\n",
        "    max_pool4 = tf.keras.layers.GlobalMaxPooling1D()(actv4)\r\n",
        "\r\n",
        "\r\n",
        "    conv5 = Conv1D(filters=64,  kernel_size=6, padding='same')(in_id)    \r\n",
        "    batch_norm5  = BatchNormalization()(conv5)\r\n",
        "    actv5 = Activation('relu')(batch_norm5)\r\n",
        "    max_pool5 = tf.keras.layers.GlobalMaxPooling1D()(actv5)\r\n",
        "\r\n",
        "    final = tf.keras.layers.Concatenate(axis=1)([max_pool1, max_pool2, max_pool3, max_pool4, max_pool5])\r\n",
        "\r\n",
        "    drop = tf.keras.layers.Dropout(0.2)(final)\r\n",
        "\r\n",
        "    predictions = tf.keras.layers.Dense(2, activation='softmax')(drop)\r\n",
        "    model = tf.keras.Model(inputs=in_id, outputs=predictions)\r\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\r\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc'])\r\n",
        "    return model"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1e4YD0qq2nW"
      },
      "source": [
        "# from tensorflow.keras import backend as K\r\n",
        "# nb_of_sents = 30\r\n",
        "# def getModel(LSTM_CELL_SIZE):\r\n",
        "#     in_id = tf.keras.layers.Input((nb_of_sents, 768), name=\"input_shape\")\r\n",
        "#     lstm_later= tf.keras.layers.LSTM(LSTM_CELL_SIZE)(in_id)\r\n",
        "#     drop = tf.keras.layers.Dropout(0.2)(lstm_later)\r\n",
        "#     predictions = tf.keras.layers.Dense(2, activation='softmax')(drop)\r\n",
        "#     model = tf.keras.Model(inputs=in_id, outputs=predictions)\r\n",
        "#     opt = tf.keras.optimizers.Adam(learning_rate=0.001)\r\n",
        "#     model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc'])\r\n",
        "#     return model\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:50:48.008526Z",
          "start_time": "2020-03-05T14:50:47.762964Z"
        },
        "code_folding": [],
        "scrolled": true,
        "id": "wHj4okK8JjTH"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "nb_of_sents = 30\n",
        "def getModel(LSTM_CELL_SIZE):\n",
        "    in_id = tf.keras.layers.Input((nb_of_sents, 768), name=\"input_shape\")\n",
        "    lstm_later, forward_h, forward_c= tf.keras.layers.LSTM(LSTM_CELL_SIZE, return_sequences=True, return_state=True)(in_id)\n",
        "    linear =  tf.keras.layers.Dense(LSTM_CELL_SIZE )(forward_h)\n",
        "    attention = tf.keras.layers.dot([lstm_later, linear], axes=(-1))\n",
        "    attention = tf.keras.layers.Activation('softmax',  name='attention_vec')(attention)\n",
        "    attention = tf.keras.layers.RepeatVector(LSTM_CELL_SIZE)(attention)\n",
        "    attention = tf.keras.layers.Permute([2, 1])(attention)\n",
        "    sent_representation = tf.keras.layers.multiply([lstm_later, attention])\n",
        "    sent_representation = tf.keras.layers.Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation)\n",
        "    sent_representation_final = tf.keras.layers.Concatenate()([sent_representation, forward_h])\n",
        "    drop = tf.keras.layers.Dropout(0.2)(sent_representation_final)\n",
        "    predictions = tf.keras.layers.Dense(2, activation='softmax')(drop)\n",
        "    model = tf.keras.Model(inputs=in_id, outputs=predictions)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc'])\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_rNbkbYsyzt",
        "outputId": "6e40ba9f-1f83-49a3-f77d-364a87903d8f"
      },
      "source": [
        "model = getModel(150)\r\n",
        "model.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_shape (InputLayer)        [(None, 30, 768)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 30, 64)       98368       input_shape[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_13 (Conv1D)              (None, 30, 64)       147520      input_shape[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_14 (Conv1D)              (None, 30, 64)       196672      input_shape[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_15 (Conv1D)              (None, 30, 64)       245824      input_shape[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_16 (Conv1D)              (None, 30, 64)       294976      input_shape[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 30, 64)       256         conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 30, 64)       256         conv1d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 30, 64)       256         conv1d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 30, 64)       256         conv1d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 30, 64)       256         conv1d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 30, 64)       0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 30, 64)       0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 30, 64)       0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 30, 64)       0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 30, 64)       0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_5 (GlobalM (None, 64)           0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_6 (GlobalM (None, 64)           0           activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_7 (GlobalM (None, 64)           0           activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_8 (GlobalM (None, 64)           0           activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d_9 (GlobalM (None, 64)           0           activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 320)          0           global_max_pooling1d_5[0][0]     \n",
            "                                                                 global_max_pooling1d_6[0][0]     \n",
            "                                                                 global_max_pooling1d_7[0][0]     \n",
            "                                                                 global_max_pooling1d_8[0][0]     \n",
            "                                                                 global_max_pooling1d_9[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 320)          0           concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 2)            642         dropout_2[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 985,282\n",
            "Trainable params: 984,642\n",
            "Non-trainable params: 640\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:50:49.503938Z",
          "start_time": "2020-03-05T14:50:49.485764Z"
        },
        "id": "rQKRq3rSJjTW"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(embedded_text, labels, test_size=0.1)\n",
        "y_train = [to_categorical(i, num_classes=2) for i in y_train]\n",
        "y_train= np.array(y_train)\n",
        "y_test = [to_categorical(i, num_classes=2) for i in y_test]\n",
        "y_test= np.array(y_test)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2S-j9EXhhNK"
      },
      "source": [
        "del embedded_text\r\n",
        "del labels"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn3NDWK3jmBj"
      },
      "source": [
        "X_train = np.array(X_train)\r\n",
        "X_test = np.array(X_test)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKn1YarzjpSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366eddb9-f55a-4dad-900f-04a6facdfd41"
      },
      "source": [
        "X_train.shape ,  X_test.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((44910, 30, 768), (4990, 30, 768))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:56:30.899243Z",
          "start_time": "2020-03-05T14:52:53.158245Z"
        },
        "id": "Mb6eGPQEJjTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59a7d4b-0929-4599-e7da-b22be5952aa0"
      },
      "source": [
        "# print(X_train_encoded[0].shape, X_train_encoded[0].dtype)\n",
        "# print(X_train_encoded[1].shape, X_train_encoded[1].dtype)\n",
        "# print(X_train_encoded[2].shape, X_train_encoded[2].dtype)\n",
        "print(y_train.shape, y_train.dtype)\n",
        "print(y_test.shape, y_test.dtype)\n",
        "\n",
        "# model.fit(X_train_encoded, y_train, shuffle=True, epochs=3, batch_size=32, validation_data=(X_test_encoded, y_test),\n",
        "#          sample_weight=sample_weights)\n",
        "\n",
        "model.fit(X_train, y_train, shuffle=True, epochs=8, batch_size=64, validation_data=(X_test, y_test))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(44910, 2) float32\n",
            "(4990, 2) float32\n",
            "Epoch 1/8\n",
            "702/702 [==============================] - 11s 12ms/step - loss: 0.6649 - acc: 0.6742 - val_loss: 0.3917 - val_acc: 0.8357\n",
            "Epoch 2/8\n",
            "702/702 [==============================] - 8s 12ms/step - loss: 0.4291 - acc: 0.7968 - val_loss: 0.3639 - val_acc: 0.8433\n",
            "Epoch 3/8\n",
            "702/702 [==============================] - 8s 11ms/step - loss: 0.3793 - acc: 0.8255 - val_loss: 0.3422 - val_acc: 0.8531\n",
            "Epoch 4/8\n",
            "702/702 [==============================] - 8s 11ms/step - loss: 0.3538 - acc: 0.8419 - val_loss: 0.3217 - val_acc: 0.8597\n",
            "Epoch 5/8\n",
            "702/702 [==============================] - 8s 12ms/step - loss: 0.3243 - acc: 0.8542 - val_loss: 0.3208 - val_acc: 0.8551\n",
            "Epoch 6/8\n",
            "702/702 [==============================] - 8s 11ms/step - loss: 0.3034 - acc: 0.8655 - val_loss: 0.3101 - val_acc: 0.8621\n",
            "Epoch 7/8\n",
            "702/702 [==============================] - 8s 11ms/step - loss: 0.2939 - acc: 0.8702 - val_loss: 0.3123 - val_acc: 0.8603\n",
            "Epoch 8/8\n",
            "702/702 [==============================] - 7s 11ms/step - loss: 0.2762 - acc: 0.8806 - val_loss: 0.3633 - val_acc: 0.8325\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f012ce54198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkXW0cMlgOZK"
      },
      "source": [
        "# model.save_weights('/content/drive/MyDrive/folder_me/model_weights/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eguog315gbnn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:56:34.833293Z",
          "start_time": "2020-03-05T14:56:30.904077Z"
        },
        "id": "XNqCvMoJJjTt"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "test_pred = model.predict(X_test_encoded)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:56:34.879740Z",
          "start_time": "2020-03-05T14:56:34.835867Z"
        },
        "id": "Iuv8MJ4bJjTw"
      },
      "source": [
        "idx2tag = {i: w for w, i in tag2idx.items()}\n",
        "\n",
        "def pred2label(pred):\n",
        "    out = []\n",
        "    for pred_i in pred:\n",
        "        out_i = []\n",
        "        for p in pred_i:\n",
        "            p_i = np.argmax(p)\n",
        "            out_i.append(idx2tag[p_i].replace(\"PAD\", \"O\"))\n",
        "        out.append(out_i)\n",
        "    return out\n",
        "    \n",
        "pred_labels = pred2label(test_pred)\n",
        "test_labels = pred2label(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:56:34.935733Z",
          "start_time": "2020-03-05T14:56:34.881149Z"
        },
        "id": "MLqVzIrKJjTy"
      },
      "source": [
        "pred_labels = np.reshape(pred_labels, (-1,))\n",
        "test_labels = np.reshape(test_labels, (-1,))\n",
        "print(classification_report(test_labels, pred_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATtwkm6wJjT0"
      },
      "source": [
        "# model.save_weights('/content/drive/My Drive/Colab Notebooks/model_weights/weights_6_dense_min/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OEw4T72JjT3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pADvP4AsJjT6"
      },
      "source": [
        "#42 f1 on 2 min"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}