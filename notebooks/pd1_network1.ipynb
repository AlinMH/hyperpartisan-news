{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "tf_env_gpu",
      "language": "python",
      "name": "tf_env_gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Copy of Copy of documents_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:50:47.759753Z",
          "start_time": "2020-03-05T14:50:45.744863Z"
        },
        "id": "IO3xMos3JjSt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f9fe5d-40de-432f-887e-2636a01182a6"
      },
      "source": [
        "!pip install transformers==3.2.0\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import statistics\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Conv1D, Dropout, Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.2.0 in /usr/local/lib/python3.6/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (1.18.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (0.1.94)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (20.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (0.8.1rc2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.2.0) (0.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.2.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.2.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.2.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.2.0) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.2.0) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.2.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.2.0) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.2.0) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMc6GXcI7ZmI",
        "outputId": "d5b8bd71-bc4e-44dd-f311-2e347cfcba6a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pupMiV7xK7Xp"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Conv1D, Dropout, Embedding\r\n",
        "from tensorflow.keras.preprocessing import sequence\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from transformers import BertTokenizer, TFBertModel\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "import nltk\r\n",
        "import tensorflow_hub as hub\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji7H_igvmKUz"
      },
      "source": [
        "from gensim.parsing import preprocessing\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKXUOYR_pkDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0431672d-b97e-4147-8a32-ff55317e866b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj2Rp_phJjSz"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9HbU9ZUJjS4"
      },
      "source": [
        "path_to_file = '/content/drive/MyDrive/test_all.csv'\r\n",
        "\r\n",
        "df = pd.read_csv(path_to_file)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "UUKJ6MIe7gQQ",
        "outputId": "ba913c2e-af68-4547-fddd-ff267698d146"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>hyperpartisan</th>\n",
              "      <th>bias</th>\n",
              "      <th>url</th>\n",
              "      <th>labeled-by</th>\n",
              "      <th>published-at</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17</td>\n",
              "      <td>False</td>\n",
              "      <td>left-center</td>\n",
              "      <td>http://sfgate.com/politics/article/SAN-FRANCIS...</td>\n",
              "      <td>publisher</td>\n",
              "      <td>2004-07-29</td>\n",
              "      <td>SAN FRANCISCO / Head of Juvenile Probation Dep...</td>\n",
              "      <td>Chief juvenile probation officer  Gwendolyn Tu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23</td>\n",
              "      <td>True</td>\n",
              "      <td>right</td>\n",
              "      <td>https://thecollegefix.com/post/32204/</td>\n",
              "      <td>publisher</td>\n",
              "      <td>2017-04-20</td>\n",
              "      <td>University leaders ban pro-life flag display, ...</td>\n",
              "      <td>Pro-life students at Wilfrid Laurier Universit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29</td>\n",
              "      <td>True</td>\n",
              "      <td>left</td>\n",
              "      <td>https://wonkette.com/622815/donald-trump-get-y...</td>\n",
              "      <td>publisher</td>\n",
              "      <td>2017-09-12</td>\n",
              "      <td>DONALD TRUMP, GET YOUR TINY PIGGY PERVERT HAND...</td>\n",
              "      <td>Time for Hillary Clinton to go back to Benghaz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32</td>\n",
              "      <td>True</td>\n",
              "      <td>right</td>\n",
              "      <td>http://thegatewaypundit.com/2016/06/dirty-croo...</td>\n",
              "      <td>publisher</td>\n",
              "      <td>2016-06-05</td>\n",
              "      <td>DIRTY: Hillary Clinton Implies Trump is a Nazi...</td>\n",
              "      <td>Do you want to know why violent unhinged lefti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>37</td>\n",
              "      <td>False</td>\n",
              "      <td>right-center</td>\n",
              "      <td>https://cfr.org/report/bipartisan-work-plan</td>\n",
              "      <td>publisher</td>\n",
              "      <td>2015-01-12</td>\n",
              "      <td>A Bipartisan Work Plan</td>\n",
              "      <td>American workers continue to struggle with los...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                               body\n",
              "0  17  ...  Chief juvenile probation officer  Gwendolyn Tu...\n",
              "1  23  ...  Pro-life students at Wilfrid Laurier Universit...\n",
              "2  29  ...  Time for Hillary Clinton to go back to Benghaz...\n",
              "3  32  ...  Do you want to know why violent unhinged lefti...\n",
              "4  37  ...  American workers continue to struggle with los...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e1sh7m61_-A"
      },
      "source": [
        "df_hyper_false = df[df.hyperpartisan == False]\r\n",
        "df_hyper_true = df[df.hyperpartisan == True]\r\n",
        "\r\n",
        "text_values_false = df_hyper_false.body.values\r\n",
        "text_values_true = df_hyper_true.body.values\r\n",
        "tragets_false = df_hyper_false.hyperpartisan.values\r\n",
        "tragets_true = df_hyper_true.hyperpartisan.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18227BEIuacg"
      },
      "source": [
        "dataset_size_div_by_2 = 50_000\r\n",
        "\r\n",
        "text_values_false = text_values_false[:dataset_size_div_by_2]\r\n",
        "tragets_false = tragets_false[:dataset_size_div_by_2]\r\n",
        "#############################\r\n",
        "text_values_true = text_values_true[:dataset_size_div_by_2]\r\n",
        "tragets_true = tragets_true[:dataset_size_div_by_2]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "text = np.concatenate((text_values_false, text_values_true), axis=0)\r\n",
        "labels = np.concatenate((tragets_false, tragets_true), axis=0)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyDqngBtlvFK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "e21D8jtOJjS7"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOrFMtpLJjS_"
      },
      "source": [
        "sent_text = []\r\n",
        "new_labels = []\r\n",
        "for idx, txt in enumerate(text):\r\n",
        "  if type(txt) == str:\r\n",
        "    my_txt = txt.replace('\\n', ' ').lower()\r\n",
        "    # my_txt = preprocessing.remove_stopwords(my_txt)\r\n",
        "    # my_txt = preprocessing.strip_tags(preprocessing.strip_numeric(my_txt))\r\n",
        "    sents = sent_tokenize(txt)\r\n",
        "    word_level_sents = []\r\n",
        "    for sent in sents:\r\n",
        "      word_level_sents.append(word_tokenize(sent))\r\n",
        "    sent_text.append(word_level_sents)\r\n",
        "    new_labels.append(labels[idx])\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0JeDkw8f_gJ"
      },
      "source": [
        "size_sent = []\r\n",
        "size_word = []\r\n",
        "for txt in sent_text:\r\n",
        "  size_sent.append(len(txt))\r\n",
        "  for val in txt:\r\n",
        "    size_word.append(len(val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMzI5yiAg0Qf",
        "outputId": "a134cc42-c751-4ca1-ac8a-cb8f0f2b536a"
      },
      "source": [
        "np.mean(size_sent), np.mean(size_word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28.96020482662746, 28.483690329336063)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQcYJs30WLtP"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "oov_token = '<UNK>'\r\n",
        "pad_type = 'post'\r\n",
        "trunc_type = 'post'\r\n",
        "num_words = 35_000\r\n",
        "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\r\n",
        "to_pass_token = []\r\n",
        "for idx, doc in enumerate(sent_text):\r\n",
        "  for sent in doc:\r\n",
        "    to_pass_token.append(' '.join(sent))\r\n",
        "\r\n",
        "tokenizer.fit_on_texts(to_pass_token)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIsl0uKlWdv6"
      },
      "source": [
        "transf_seq = []\r\n",
        "max_seq_lenght = 30\r\n",
        "max_sent_lenght = 20\r\n",
        "for idx, doc in enumerate(sent_text):\r\n",
        "  transf_seq.append([])\r\n",
        "  for idx, sent in enumerate(doc):\r\n",
        "    if idx == max_sent_lenght:\r\n",
        "      break\r\n",
        "    train_sequences = tokenizer.texts_to_sequences([' '.join(sent)])\r\n",
        "    train_padded = pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=max_seq_lenght)\r\n",
        "    transf_seq[-1].append(train_padded[0])\r\n",
        "  while len(transf_seq[-1]) != max_sent_lenght:\r\n",
        "    train_padded = pad_sequences([[],[]], padding=pad_type, truncating=trunc_type, maxlen=max_seq_lenght)\r\n",
        "    transf_seq[-1].append(train_padded[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OpmwCuPh0zX"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:50:48.008526Z",
          "start_time": "2020-03-05T14:50:47.762964Z"
        },
        "code_folding": [],
        "scrolled": true,
        "id": "wHj4okK8JjTH"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def getModel(LSTM_CELL_SIZE):\n",
        "    in_id = tf.keras.layers.Input((max_sent_lenght, max_seq_lenght), dtype='int32', name=\"input_shape\")\n",
        "    emb_words = tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(input_dim=num_words + 1, output_dim=32))(in_id)\n",
        "    averaged_sents = tf.keras.layers.TimeDistributed(tf.keras.layers.GlobalAveragePooling1D())(emb_words)\n",
        "    lstm_later, forward_h, forward_c= tf.keras.layers.LSTM(LSTM_CELL_SIZE, return_sequences=True, return_state=True)(averaged_sents)\n",
        "    linear =  tf.keras.layers.Dense(LSTM_CELL_SIZE )(forward_h)\n",
        "    attention = tf.keras.layers.dot([lstm_later, linear], axes=(-1))\n",
        "    attention = tf.keras.layers.Activation('softmax',  name='attention_vec')(attention)\n",
        "    attention = tf.keras.layers.RepeatVector(LSTM_CELL_SIZE)(attention)\n",
        "    attention = tf.keras.layers.Permute([2, 1])(attention)\n",
        "    sent_representation = tf.keras.layers.multiply([lstm_later, attention])\n",
        "    sent_representation = tf.keras.layers.Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation)\n",
        "    sent_representation_final = tf.keras.layers.Concatenate()([sent_representation, forward_h])\n",
        "    drop = tf.keras.layers.Dropout(0.2)(sent_representation)\n",
        "    predictions = tf.keras.layers.Dense(2, activation='softmax')(drop)\n",
        "    model = tf.keras.Model(inputs=in_id, outputs=predictions)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc'])\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:50:48.023169Z",
          "start_time": "2020-03-05T14:50:48.010485Z"
        },
        "id": "yEVQTj8tJjTL"
      },
      "source": [
        "model = getModel(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLaNf2EIi1JQ",
        "outputId": "9bf3e0ee-bea6-429f-8404-122526904b46"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_shape (InputLayer)        [(None, 20, 30)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_6 (TimeDistrib (None, 20, 30, 32)   1120032     input_shape[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_7 (TimeDistrib (None, 20, 32)       0           time_distributed_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, 20, 100), (N 53200       time_distributed_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 100)          10100       lstm_3[0][1]                     \n",
            "__________________________________________________________________________________________________\n",
            "dot_3 (Dot)                     (None, 20)           0           lstm_3[0][0]                     \n",
            "                                                                 dense_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_vec (Activation)      (None, 20)           0           dot_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_3 (RepeatVector)  (None, 100, 20)      0           attention_vec[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "permute_3 (Permute)             (None, 20, 100)      0           repeat_vector_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "multiply_3 (Multiply)           (None, 20, 100)      0           lstm_3[0][0]                     \n",
            "                                                                 permute_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 100)          0           multiply_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 100)          0           lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 2)            202         dropout_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,183,534\n",
            "Trainable params: 1,183,534\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:50:49.503938Z",
          "start_time": "2020-03-05T14:50:49.485764Z"
        },
        "id": "rQKRq3rSJjTW"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(transf_seq, new_labels, test_size=0.1)\n",
        "y_train = [to_categorical(i, num_classes=2) for i in y_train]\n",
        "y_train= np.array(y_train)\n",
        "y_test = [to_categorical(i, num_classes=2) for i in y_test]\n",
        "y_test= np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn3NDWK3jmBj"
      },
      "source": [
        "X_train = np.array(X_train)\r\n",
        "X_test = np.array(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKn1YarzjpSH",
        "outputId": "7fe02343-6fc0-497e-d722-7c205f655d6d"
      },
      "source": [
        "X_train.shape ,  X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((89988, 20, 30), (9999, 20, 30))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:56:30.899243Z",
          "start_time": "2020-03-05T14:52:53.158245Z"
        },
        "id": "Mb6eGPQEJjTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8769cd8b-e646-491f-97ab-a0ba42c80fb9"
      },
      "source": [
        "# print(X_train_encoded[0].shape, X_train_encoded[0].dtype)\n",
        "# print(X_train_encoded[1].shape, X_train_encoded[1].dtype)\n",
        "# print(X_train_encoded[2].shape, X_train_encoded[2].dtype)\n",
        "print(y_train.shape, y_train.dtype)\n",
        "print(y_test.shape, y_test.dtype)\n",
        "\n",
        "# model.fit(X_train_encoded, y_train, shuffle=True, epochs=3, batch_size=32, validation_data=(X_test_encoded, y_test),\n",
        "#          sample_weight=sample_weights)\n",
        "\n",
        "model.fit(X_train, y_train, shuffle=True, epochs=5, batch_size=64, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(89988, 2) float32\n",
            "(9999, 2) float32\n",
            "Epoch 1/5\n",
            "1407/1407 [==============================] - 79s 56ms/step - loss: 0.4741 - acc: 0.7479 - val_loss: 0.3554 - val_acc: 0.8377\n",
            "Epoch 2/5\n",
            "1407/1407 [==============================] - 79s 56ms/step - loss: 0.3190 - acc: 0.8566 - val_loss: 0.3353 - val_acc: 0.8502\n",
            "Epoch 3/5\n",
            "1407/1407 [==============================] - 79s 56ms/step - loss: 0.2728 - acc: 0.8768 - val_loss: 0.3398 - val_acc: 0.8469\n",
            "Epoch 4/5\n",
            "1407/1407 [==============================] - 80s 57ms/step - loss: 0.2397 - acc: 0.8908 - val_loss: 0.3527 - val_acc: 0.8414\n",
            "Epoch 5/5\n",
            "1407/1407 [==============================] - 80s 57ms/step - loss: 0.2172 - acc: 0.9000 - val_loss: 0.3466 - val_acc: 0.8561\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe2b97fcb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:56:34.833293Z",
          "start_time": "2020-03-05T14:56:30.904077Z"
        },
        "id": "XNqCvMoJJjTt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "dd489f46-1291-4bca-f304-15f0a31f559a"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "test_pred = model.predict(X_test_encoded)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-e6cec7a5bc0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'X_test_encoded' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:56:34.879740Z",
          "start_time": "2020-03-05T14:56:34.835867Z"
        },
        "id": "Iuv8MJ4bJjTw"
      },
      "source": [
        "idx2tag = {i: w for w, i in tag2idx.items()}\n",
        "\n",
        "def pred2label(pred):\n",
        "    out = []\n",
        "    for pred_i in pred:\n",
        "        out_i = []\n",
        "        for p in pred_i:\n",
        "            p_i = np.argmax(p)\n",
        "            out_i.append(idx2tag[p_i].replace(\"PAD\", \"O\"))\n",
        "        out.append(out_i)\n",
        "    return out\n",
        "    \n",
        "pred_labels = pred2label(test_pred)\n",
        "test_labels = pred2label(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:56:34.935733Z",
          "start_time": "2020-03-05T14:56:34.881149Z"
        },
        "id": "MLqVzIrKJjTy"
      },
      "source": [
        "pred_labels = np.reshape(pred_labels, (-1,))\n",
        "test_labels = np.reshape(test_labels, (-1,))\n",
        "print(classification_report(test_labels, pred_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATtwkm6wJjT0"
      },
      "source": [
        "# model.save_weights('/content/drive/My Drive/Colab Notebooks/model_weights/weights_6_dense_min/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OEw4T72JjT3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pADvP4AsJjT6"
      },
      "source": [
        "#42 f1 on 2 min"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}