{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "tf_env_gpu",
      "language": "python",
      "name": "tf_env_gpu"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Copy of Copy of documents_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:50:47.759753Z",
          "start_time": "2020-03-05T14:50:45.744863Z"
        },
        "id": "IO3xMos3JjSt"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import statistics\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Conv1D, Dropout, Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMc6GXcI7ZmI",
        "outputId": "d7582cec-33fd-4dd9-ba7c-91047d7533b0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pupMiV7xK7Xp"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Conv1D, Dropout, Embedding\r\n",
        "from tensorflow.keras.preprocessing import sequence\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from transformers import BertTokenizer, TFBertModel\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "import nltk\r\n",
        "import tensorflow_hub as hub\r\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji7H_igvmKUz"
      },
      "source": [
        "from gensim.parsing import preprocessing\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKXUOYR_pkDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fabf5ce-20f6-4756-be78-e80870576ed3"
      },
      "source": [
        "\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj2Rp_phJjSz"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import pandas as pd"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9HbU9ZUJjS4"
      },
      "source": [
        "#PATH TO DATA FISIER test_all.csv pe care l-am incercat pe teams\r\n",
        "\r\n",
        "path_to_file = '/content/drive/MyDrive/dataset_SRI/test_all.csv'\r\n",
        "\r\n",
        "df = pd.read_csv(path_to_file)\r\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "UUKJ6MIe7gQQ",
        "outputId": "d7840c7c-5e64-441a-8708-809d36a58b00"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>hyperpartisan</th>\n",
              "      <th>bias</th>\n",
              "      <th>url</th>\n",
              "      <th>labeled-by</th>\n",
              "      <th>published-at</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17</td>\n",
              "      <td>False</td>\n",
              "      <td>left-center</td>\n",
              "      <td>http://sfgate.com/politics/article/SAN-FRANCIS...</td>\n",
              "      <td>publisher</td>\n",
              "      <td>2004-07-29</td>\n",
              "      <td>SAN FRANCISCO / Head of Juvenile Probation Dep...</td>\n",
              "      <td>Chief juvenile probation officer  Gwendolyn Tu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23</td>\n",
              "      <td>True</td>\n",
              "      <td>right</td>\n",
              "      <td>https://thecollegefix.com/post/32204/</td>\n",
              "      <td>publisher</td>\n",
              "      <td>2017-04-20</td>\n",
              "      <td>University leaders ban pro-life flag display, ...</td>\n",
              "      <td>Pro-life students at Wilfrid Laurier Universit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29</td>\n",
              "      <td>True</td>\n",
              "      <td>left</td>\n",
              "      <td>https://wonkette.com/622815/donald-trump-get-y...</td>\n",
              "      <td>publisher</td>\n",
              "      <td>2017-09-12</td>\n",
              "      <td>DONALD TRUMP, GET YOUR TINY PIGGY PERVERT HAND...</td>\n",
              "      <td>Time for Hillary Clinton to go back to Benghaz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32</td>\n",
              "      <td>True</td>\n",
              "      <td>right</td>\n",
              "      <td>http://thegatewaypundit.com/2016/06/dirty-croo...</td>\n",
              "      <td>publisher</td>\n",
              "      <td>2016-06-05</td>\n",
              "      <td>DIRTY: Hillary Clinton Implies Trump is a Nazi...</td>\n",
              "      <td>Do you want to know why violent unhinged lefti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>37</td>\n",
              "      <td>False</td>\n",
              "      <td>right-center</td>\n",
              "      <td>https://cfr.org/report/bipartisan-work-plan</td>\n",
              "      <td>publisher</td>\n",
              "      <td>2015-01-12</td>\n",
              "      <td>A Bipartisan Work Plan</td>\n",
              "      <td>American workers continue to struggle with los...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                               body\n",
              "0  17  ...  Chief juvenile probation officer  Gwendolyn Tu...\n",
              "1  23  ...  Pro-life students at Wilfrid Laurier Universit...\n",
              "2  29  ...  Time for Hillary Clinton to go back to Benghaz...\n",
              "3  32  ...  Do you want to know why violent unhinged lefti...\n",
              "4  37  ...  American workers continue to struggle with los...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e1sh7m61_-A"
      },
      "source": [
        "df_hyper_false = df[df.hyperpartisan == False]\r\n",
        "df_hyper_true = df[df.hyperpartisan == True]\r\n",
        "\r\n",
        "text_values_false = df_hyper_false.body.values\r\n",
        "text_values_true = df_hyper_true.body.values\r\n",
        "tragets_false = df_hyper_false.hyperpartisan.values\r\n",
        "tragets_true = df_hyper_true.hyperpartisan.values"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18227BEIuacg"
      },
      "source": [
        "dataset_size_div_by_2 = 50_000\r\n",
        "\r\n",
        "text_values_false = text_values_false[:dataset_size_div_by_2]\r\n",
        "tragets_false = tragets_false[:dataset_size_div_by_2]\r\n",
        "#############################\r\n",
        "text_values_true = text_values_true[:dataset_size_div_by_2]\r\n",
        "tragets_true = tragets_true[:dataset_size_div_by_2]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "text = np.concatenate((text_values_false, text_values_true), axis=0)\r\n",
        "labels = np.concatenate((tragets_false, tragets_true), axis=0)\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyDqngBtlvFK"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "e21D8jtOJjS7"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOrFMtpLJjS_"
      },
      "source": [
        "sent_text = []\r\n",
        "new_labels = []\r\n",
        "for idx, txt in enumerate(text):\r\n",
        "  if type(txt) == str:\r\n",
        "    my_txt = txt.replace('\\n', ' ').lower()\r\n",
        "    # my_txt = preprocessing.remove_stopwords(my_txt)\r\n",
        "    # my_txt = preprocessing.strip_tags(preprocessing.strip_numeric(my_txt))\r\n",
        "    sents = sent_tokenize(txt)\r\n",
        "    word_level_sents = []\r\n",
        "    for sent in sents:\r\n",
        "      word_level_sents.append(word_tokenize(sent))\r\n",
        "    sent_text.append(word_level_sents)\r\n",
        "    new_labels.append(labels[idx])\r\n",
        "\r\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0JeDkw8f_gJ"
      },
      "source": [
        "size_sent = []\r\n",
        "size_word = []\r\n",
        "for txt in sent_text:\r\n",
        "  size_sent.append(len(txt))\r\n",
        "  for val in txt:\r\n",
        "    size_word.append(len(val))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7ou8e5L7hNw"
      },
      "source": [
        "nb_of_tokens = 60\r\n",
        "nb_of_sents = 30\r\n",
        "\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMzI5yiAg0Qf",
        "outputId": "59309cdf-f8ec-4724-fc1a-e0486161d90d"
      },
      "source": [
        "np.mean(size_sent), np.mean(size_word)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28.96020482662746, 28.483690329336063)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8uT6A7uuskz",
        "outputId": "98b8705d-bf5d-49d2-d21f-6bf53e9c0c5f"
      },
      "source": [
        "!pip install transformers\r\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.95)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mCUmLQEu6F-",
        "outputId": "a6f37749-7cae-4b5d-f8dc-b7b888b4cdf5"
      },
      "source": [
        "####get new sentences :D:D:D:D:D\r\n",
        "\r\n",
        "from transformers import BertTokenizer, TFBertModel\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\r\n",
        "model = TFBertModel.from_pretrained(\"bert-base-uncased\")\r\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xaDISaL4p93"
      },
      "source": [
        "def create_bert_ds(sent_text):\r\n",
        "  bert_encodings = []\r\n",
        "  for idx2, a_news in enumerate(sent_text):\r\n",
        "    if idx2 == 10:\r\n",
        "      print(idx2)\r\n",
        "      break\r\n",
        "    snt_good = []\r\n",
        "    for idx, snt in enumerate(a_news):\r\n",
        "      if idx == nb_of_sents:\r\n",
        "        break\r\n",
        "      snt_good.append(' '.join(snt))\r\n",
        "    while len(snt_good) != nb_of_sents:\r\n",
        "      snt_good.append('')\r\n",
        "    assert(len(snt_good) == nb_of_sents)\r\n",
        "    encoded_inputs = tokenizer(snt_good,  padding=True, truncation=True, return_tensors=\"tf\", max_length=60)\r\n",
        "    output = model(encoded_inputs)\r\n",
        "    bert_encodings.append(output[0])\r\n",
        "  return bert_encodings\r\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMJOR5ZF516P",
        "outputId": "3453a61d-9c56-4ff4-8e8b-5854c519a2b5"
      },
      "source": [
        "encodings = create_bert_ds(sent_text)\r\n",
        "#TO DO HERE TO SAVE FILE TO PATH\r\n",
        "with open('TODO ADD PATH HERE, 'wb') as filehandle:\r\n",
        "    # store the data as binary data stream\r\n",
        "    pickle.dump(encodings, filehandle)\r\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4f31n-AOYmQ",
        "outputId": "9e33770c-0c0f-4c74-ddbc-1e86c393cbbe"
      },
      "source": [
        "##################################\r\n",
        "STOP RUN ALL HERE"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQcYJs30WLtP"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "oov_token = '<UNK>'\r\n",
        "pad_type = 'post'\r\n",
        "trunc_type = 'post'\r\n",
        "num_words = 35_000\r\n",
        "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\r\n",
        "to_pass_token = []\r\n",
        "for idx, doc in enumerate(sent_text):\r\n",
        "  for sent in doc:\r\n",
        "    to_pass_token.append(' '.join(sent))\r\n",
        "\r\n",
        "tokenizer.fit_on_texts(to_pass_token)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIsl0uKlWdv6"
      },
      "source": [
        "transf_seq = []\r\n",
        "max_seq_lenght = 30\r\n",
        "max_sent_lenght = 20\r\n",
        "for idx, doc in enumerate(sent_text):\r\n",
        "  transf_seq.append([])\r\n",
        "  for idx, sent in enumerate(doc):\r\n",
        "    if idx == max_sent_lenght:\r\n",
        "      break\r\n",
        "    train_sequences = tokenizer.texts_to_sequences([' '.join(sent)])\r\n",
        "    train_padded = pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=max_seq_lenght)\r\n",
        "    transf_seq[-1].append(train_padded[0])\r\n",
        "  while len(transf_seq[-1]) != max_sent_lenght:\r\n",
        "    train_padded = pad_sequences([[],[]], padding=pad_type, truncating=trunc_type, maxlen=max_seq_lenght)\r\n",
        "    transf_seq[-1].append(train_padded[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OpmwCuPh0zX"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:50:48.008526Z",
          "start_time": "2020-03-05T14:50:47.762964Z"
        },
        "code_folding": [],
        "scrolled": true,
        "id": "wHj4okK8JjTH"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def getModel(LSTM_CELL_SIZE):\n",
        "    in_id = tf.keras.layers.Input((max_sent_lenght, max_seq_lenght), dtype='int32', name=\"input_shape\")\n",
        "    emb_words = tf.keras.layers.TimeDistributed(tf.keras.layers.Embedding(input_dim=num_words + 1, output_dim=32))(in_id)\n",
        "    averaged_sents = tf.keras.layers.TimeDistributed(tf.keras.layers.GlobalAveragePooling1D())(emb_words)\n",
        "    lstm_later, forward_h, forward_c= tf.keras.layers.LSTM(LSTM_CELL_SIZE, return_sequences=True, return_state=True)(averaged_sents)\n",
        "    linear =  tf.keras.layers.Dense(LSTM_CELL_SIZE )(forward_h)\n",
        "    attention = tf.keras.layers.dot([lstm_later, linear], axes=(-1))\n",
        "    attention = tf.keras.layers.Activation('softmax',  name='attention_vec')(attention)\n",
        "    attention = tf.keras.layers.RepeatVector(LSTM_CELL_SIZE)(attention)\n",
        "    attention = tf.keras.layers.Permute([2, 1])(attention)\n",
        "    sent_representation = tf.keras.layers.multiply([lstm_later, attention])\n",
        "    sent_representation = tf.keras.layers.Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation)\n",
        "    sent_representation_final = tf.keras.layers.Concatenate()([sent_representation, forward_h])\n",
        "    drop = tf.keras.layers.Dropout(0.2)(sent_representation)\n",
        "    predictions = tf.keras.layers.Dense(2, activation='softmax')(drop)\n",
        "    model = tf.keras.Model(inputs=in_id, outputs=predictions)\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc'])\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:50:48.023169Z",
          "start_time": "2020-03-05T14:50:48.010485Z"
        },
        "id": "yEVQTj8tJjTL"
      },
      "source": [
        "model = getModel(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLaNf2EIi1JQ"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:50:49.503938Z",
          "start_time": "2020-03-05T14:50:49.485764Z"
        },
        "id": "rQKRq3rSJjTW"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(transf_seq, new_labels, test_size=0.1)\n",
        "y_train = [to_categorical(i, num_classes=2) for i in y_train]\n",
        "y_train= np.array(y_train)\n",
        "y_test = [to_categorical(i, num_classes=2) for i in y_test]\n",
        "y_test= np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn3NDWK3jmBj"
      },
      "source": [
        "X_train = np.array(X_train)\r\n",
        "X_test = np.array(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKn1YarzjpSH"
      },
      "source": [
        "X_train.shape ,  X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:56:30.899243Z",
          "start_time": "2020-03-05T14:52:53.158245Z"
        },
        "id": "Mb6eGPQEJjTq"
      },
      "source": [
        "# print(X_train_encoded[0].shape, X_train_encoded[0].dtype)\n",
        "# print(X_train_encoded[1].shape, X_train_encoded[1].dtype)\n",
        "# print(X_train_encoded[2].shape, X_train_encoded[2].dtype)\n",
        "print(y_train.shape, y_train.dtype)\n",
        "print(y_test.shape, y_test.dtype)\n",
        "\n",
        "# model.fit(X_train_encoded, y_train, shuffle=True, epochs=3, batch_size=32, validation_data=(X_test_encoded, y_test),\n",
        "#          sample_weight=sample_weights)\n",
        "\n",
        "model.fit(X_train, y_train, shuffle=True, epochs=5, batch_size=64, validation_data=(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:56:34.833293Z",
          "start_time": "2020-03-05T14:56:30.904077Z"
        },
        "id": "XNqCvMoJJjTt"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "test_pred = model.predict(X_test_encoded)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:56:34.879740Z",
          "start_time": "2020-03-05T14:56:34.835867Z"
        },
        "id": "Iuv8MJ4bJjTw"
      },
      "source": [
        "idx2tag = {i: w for w, i in tag2idx.items()}\n",
        "\n",
        "def pred2label(pred):\n",
        "    out = []\n",
        "    for pred_i in pred:\n",
        "        out_i = []\n",
        "        for p in pred_i:\n",
        "            p_i = np.argmax(p)\n",
        "            out_i.append(idx2tag[p_i].replace(\"PAD\", \"O\"))\n",
        "        out.append(out_i)\n",
        "    return out\n",
        "    \n",
        "pred_labels = pred2label(test_pred)\n",
        "test_labels = pred2label(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-05T14:56:34.935733Z",
          "start_time": "2020-03-05T14:56:34.881149Z"
        },
        "id": "MLqVzIrKJjTy"
      },
      "source": [
        "pred_labels = np.reshape(pred_labels, (-1,))\n",
        "test_labels = np.reshape(test_labels, (-1,))\n",
        "print(classification_report(test_labels, pred_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATtwkm6wJjT0"
      },
      "source": [
        "# model.save_weights('/content/drive/My Drive/Colab Notebooks/model_weights/weights_6_dense_min/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OEw4T72JjT3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pADvP4AsJjT6"
      },
      "source": [
        "#42 f1 on 2 min"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}